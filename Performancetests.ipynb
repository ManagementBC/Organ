{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBuMai0j6Z6UF3gP2vk0iC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManagementBC/Organ/blob/main/Performancetests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfnL594wVqgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "\n",
        "# ‚úÖ OpenAI API Key\n",
        "openai.api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "# ‚úÖ Fine-tuned model ID\n",
        "MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal:organdonations:AmbvC8Vu\"\n",
        "\n",
        "# ‚úÖ Define test donor cases\n",
        "donor_test_cases = [1, 2, 3, 4, 5]  # Example donor IDs\n",
        "\n",
        "# ‚úÖ Function to get response time\n",
        "def measure_response_time(donor_id):\n",
        "    prompt = {\n",
        "        \"donor_id\": donor_id,\n",
        "        \"recipients\": [{\"recipient_id\": 10}, {\"recipient_id\": 20}],  # Example recipient data\n",
        "    }\n",
        "\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=MODEL_ID,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI designed to match organ donors to recipients. Return only JSON output.\"},\n",
        "            {\"role\": \"user\", \"content\": str(prompt)},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    end_time = time.time()  # End timing\n",
        "    response_time = round(end_time - start_time, 3)  # Calculate elapsed time (in seconds)\n",
        "\n",
        "    return response_time, response  # Return response time and response\n",
        "\n",
        "# ‚úÖ Measure response time for 5 different donor cases\n",
        "response_times = []\n",
        "for donor in donor_test_cases:\n",
        "    response_time, response = measure_response_time(donor)\n",
        "    response_times.append(response_time)\n",
        "    print(f\"‚è≥ Response Time for Donor {donor}: {response_time} seconds\")\n",
        "\n",
        "# ‚úÖ Calculate and display the average response time\n",
        "average_response_time = round(sum(response_times) / len(response_times), 3)\n",
        "print(f\"\\n‚úÖ Average Response Time over {len(donor_test_cases)} test cases: {average_response_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kaUHGTpG1--",
        "outputId": "3e09a7d2-7064-49a8-ebb0-0a13210da8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Response Time for Donor 1: 1.168 seconds\n",
            "‚è≥ Response Time for Donor 2: 0.918 seconds\n",
            "‚è≥ Response Time for Donor 3: 0.654 seconds\n",
            "‚è≥ Response Time for Donor 4: 3.94 seconds\n",
            "‚è≥ Response Time for Donor 5: 8.565 seconds\n",
            "\n",
            "‚úÖ Average Response Time over 5 test cases: 3.049 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Convert JSON responses to strings for evaluation\n",
        "reference = json.dumps(expected_output)\n",
        "candidate = json.dumps(json.loads(model_output))\n",
        "\n",
        "# Compute BLEU Score\n",
        "bleu_score = sentence_bleu([reference.split()], candidate.split())\n",
        "\n",
        "# Compute ROUGE Score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, candidate)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"üîπ BLEU Score: {bleu_score:.4f}\")\n",
        "print(f\"üîπ ROUGE-1 Score: {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
        "print(f\"üîπ ROUGE-L Score: {rouge_scores['rougeL'].fmeasure:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-K8aITaGtiB",
        "outputId": "214be137-2fbc-4d47-a2cc-9ffe306fcd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ BLEU Score: 0.8091\n",
            "üîπ ROUGE-1 Score: 0.9091\n",
            "üîπ ROUGE-L Score: 0.9091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "9le4UuSvWBJP",
        "outputId": "989e1b71-cb58-4c31-c2d3-0362b1f6d473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1c453e0df0eec771f7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1c453e0df0eec771f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Install required dependencies (if not installed)\n",
        "!pip install openai==0.28 gradio --quiet\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import gradio as gr\n",
        "\n",
        "# Set up OpenAI API key (Replace 'your-api-key' with your actual key)\n",
        "openai.api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Make sure to set this properly in Colab\n",
        "\n",
        "# Define fine-tuned model ID\n",
        "MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal:organdonations:AmbvC8Vu\"\n",
        "\n",
        "# Hardcoded recipient data (since IPFS is not set up yet)\n",
        "recipient_data = [\n",
        "    {\n",
        "        \"id\": \"R67890\",\n",
        "        \"age\": 50,\n",
        "        \"gender\": \"Female\",\n",
        "        \"blood_type\": \"O\",\n",
        "        \"hla_typing\": [\"A1\", \"A3\", \"B7\", \"B27\", \"DR15\", \"DR11\"],\n",
        "        \"geographical_location\": \"San Francisco, CA\",\n",
        "        \"waiting_time_days\": 365,\n",
        "        \"medical_urgency_status\": \"High\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"R23456\",\n",
        "        \"age\": 35,\n",
        "        \"gender\": \"Male\",\n",
        "        \"blood_type\": \"O\",\n",
        "        \"hla_typing\": [\"A1\", \"A2\", \"B7\", \"B8\", \"DR15\", \"DR4\"],\n",
        "        \"geographical_location\": \"Boston, MA\",\n",
        "        \"waiting_time_days\": 200,\n",
        "        \"medical_urgency_status\": \"Moderate\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"R81239\",\n",
        "        \"age\": 45,\n",
        "        \"gender\": \"Female\",\n",
        "        \"blood_type\": \"A\",\n",
        "        \"hla_typing\": [\"A2\", \"A3\", \"B7\", \"B44\", \"DR4\", \"DR13\"],\n",
        "        \"geographical_location\": \"Seattle, WA\",\n",
        "        \"waiting_time_days\": 500,\n",
        "        \"medical_urgency_status\": \"High\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"R99887\",\n",
        "        \"age\": 60,\n",
        "        \"gender\": \"Male\",\n",
        "        \"blood_type\": \"B\",\n",
        "        \"hla_typing\": [\"A1\", \"A11\", \"B15\", \"B44\", \"DR11\", \"DR7\"],\n",
        "        \"geographical_location\": \"Miami, FL\",\n",
        "        \"waiting_time_days\": 150,\n",
        "        \"medical_urgency_status\": \"Low\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"R55661\",\n",
        "        \"age\": 28,\n",
        "        \"gender\": \"Female\",\n",
        "        \"blood_type\": \"AB\",\n",
        "        \"hla_typing\": [\"A3\", \"A24\", \"B8\", \"B35\", \"DR1\", \"DR13\"],\n",
        "        \"geographical_location\": \"Chicago, IL\",\n",
        "        \"waiting_time_days\": 310,\n",
        "        \"medical_urgency_status\": \"Moderate\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Function to process donor input and generate matches\n",
        "def match_donor(donor_data):\n",
        "    \"\"\"\n",
        "    Sends donor and recipient data to the fine-tuned GPT-4 model for matching.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Given the donor's medical data and the recipient dataset below, identify the best two matches.\n",
        "\n",
        "    Donor Data:\n",
        "    {json.dumps(donor_data, indent=2)}\n",
        "\n",
        "    Recipients Data:\n",
        "    {json.dumps(recipient_data, indent=2)}\n",
        "\n",
        "    Provide the best matches with scores and justification.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are an AI specializing in organ donor-recipient matching.\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        result = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return result  # Returns the AI-generated response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI for interactive testing\n",
        "def gradio_interface(donor_id, age, gender, blood_type, hla_typing, location, organ_type, hepatitis_b, hiv):\n",
        "    \"\"\"\n",
        "    Captures user input, formats it into donor JSON, and processes matching.\n",
        "    \"\"\"\n",
        "    donor_data = {\n",
        "        \"id\": donor_id,\n",
        "        \"age\": int(age),\n",
        "        \"gender\": gender,\n",
        "        \"blood_type\": blood_type,\n",
        "        \"hla_typing\": hla_typing.split(\",\"),  # Convert comma-separated HLA into a list\n",
        "        \"geographical_location\": location,\n",
        "        \"organ_type\": organ_type,\n",
        "        \"serology\": {\n",
        "            \"hepatitis_b\": hepatitis_b,\n",
        "            \"hiv\": hiv\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Run the AI model with the donor data\n",
        "    output = match_donor(donor_data)\n",
        "    return output\n",
        "\n",
        "# Create the Gradio UI\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Donor ID\"),\n",
        "        gr.Number(label=\"Age\"),\n",
        "        gr.Radio([\"Male\", \"Female\"], label=\"Gender\"),\n",
        "        gr.Textbox(label=\"Blood Type\"),\n",
        "        gr.Textbox(label=\"HLA Typing (comma-separated)\"),\n",
        "        gr.Textbox(label=\"Geographical Location\"),\n",
        "        gr.Textbox(label=\"Organ Type\"),\n",
        "        gr.Radio([\"Positive\", \"Negative\"], label=\"Hepatitis B\"),\n",
        "        gr.Radio([\"Positive\", \"Negative\"], label=\"HIV\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Organ Donation Matching System\",\n",
        "    description=\"Enter donor data and get the best matching recipients.\"\n",
        ").launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Ground truth matches (first 2 recipients for each donor)\n",
        "ground_truth = [\n",
        "    [\"R67890\", \"R23456\"],   # D001\n",
        "     [\"R67890\", \"R23456\"],   # D001\n",
        "    [\"R81239\", \"R55661\"],   # D002\n",
        "    [\"R99887\", \"R23456\"],   # D003\n",
        "    [\"R55661\", \"R81239\"],   # D004\n",
        "    [\"R23456\", \"R67890\"],    # D005\n",
        "    [\"R23456\", \"R67658\"],   # D006\n",
        "    [\"R67890\", \"R23455\"],        # D001 ‚úÖ‚úÖ\n",
        "    [\"R55661\", \"R81239\"]   # D004\n",
        "]\n",
        "\n",
        "# LLM output matches (first 2 recipients for each donor)\n",
        "llm_predictions = [\n",
        "    [\"R67890\", \"R23456\"],\n",
        "     [\"R67890\", \"R67890\"],\n",
        "    [\"R81239\", \"R81239\"],\n",
        "    [\"R99887\", \"R67890\"],\n",
        "    [\"R55661\", \"R67890\"],\n",
        "    [\"R00195_1\", \"R00195_2\"],\n",
        "    [\"R67658\", \"R67658\"],\n",
        "    [\"R67890\", \"R67890\"],\n",
        "    [\"R55661\", \"R67890\"]\n",
        "\n",
        "# Flatten and convert to binary classification: 1 if correct match, 0 otherwise\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for truth, pred in zip(ground_truth, llm_predictions):\n",
        "    for p in pred:\n",
        "        if p in truth:\n",
        "            y_true.append(1)\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_true.append(1)  # a correct match expected\n",
        "            y_pred.append(0)  # but LLM failed to match it\n",
        "\n",
        "# Fill remaining ground truth items if LLM missed them\n",
        "for truth, pred in zip(ground_truth, llm_predictions):\n",
        "    missed = len(set(truth) - set(pred))\n",
        "    for _ in range(missed):\n",
        "        y_true.append(1)\n",
        "        y_pred.append(0)\n",
        "\n",
        "# Metrics calculation\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "k0OgOhv0ZlUW",
        "outputId": "8da42679-6c15-458f-f008-2b61f22b4400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall:    0.48\n",
            "F1-score:  0.65\n"
          ]
        }
      ]
    }
  ]
}